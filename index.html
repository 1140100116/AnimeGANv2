<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>AnimeGANv2</title>
	<link rel="stylesheet" type="text/css" href="./index_files/pixl-bk.css">
	<base target="_blank"/>
</head>

<body>

<div class="roof">
	<a href="https://github.com/TachibanaYoshino/AnimeGAN"> AnimeGAN </a> →
	[Chen et al. 2019]
	</span>
</div>



<div class="content">
<div class="pageheader">
  <div class="theme"> AnimeGANv2 </div>
  <br>
  <div class="authors"> <p> Xin Chen and Gang Liu </p></div>
</div>


<div class="img1"><img src="./assets/AE86.jpg"></div>
<div class="longcaption">Figure 1. Example of photo cartoonization with our method: 
left is the real-world photo used for input, right is the animation style picture output by our proposed method.</div>

<div class="section">Abstract</div>
<p></p>
<div class="paragraph">
Transforming photos of real-world scenes into anime style images is a meaningful and challenging task in terms of computer vision and artistic style transfer. Our previously proposed AnimeGAN combines neural style transfer and generative adversarial network (GAN) to accomplish this task. However, AnimeGAN still has some obvious problems, such as high-frequency artifacts in the images generated by the model. Therefore, in this research, we propose an improved version of AnimeGAN, namely AnimeGANv2. It prevents the generation of high-frequency artifacts by simply changing the normalization of features in the network. In addition, we further reduce the scale of the generator network to achieve more efficient animation style transfer. AnimeGANv2 trained on the newly established high-quality dataset can generate animation images with better visual quality than AnimeGAN.
</div>

<div class="section">Method</div>
<p></p><div class="paragraph">
    AnimeGANv2 uses layer normalization of features to prevent the network from producing high-frequency artifacts in the generated images. However, AnimeGAN is prone to generate high-frequency artifacts due to the use of instance normalization, which is the same as the reason why styleGAN generates high-frequency artifacts. In fact, total variation loss cannot completely suppress the generation of high-frequency noise. Instance normalization is generally regarded as the best normalization method in style transfer. It can make different channels in the feature map have different feature properties, thereby promoting the diversity of styles in the images generated by the model. Layer normalization can make different channels in the feature map have the same distribution of feature properties, which can effectively prevent the generation of local noise.
	   <br><br>
	The network structure of the generator in AnimeGANv2 is shown in Figure 2. K represents the size of the convolution kernel, S represents the step size, C represents the number of convolution kernels, IRB represents the inverted residual block, resize represents the interpolation up-sampling method, and SUM means the element-wise addition. The generator parameter size of AnimeGANv2 is 8.6MB, and the generator parameter size of AnimeGAN is 15.8MB. AnimeGANv2 uses the same discriminator as AnimeGAN, the difference is that the discriminator uses layer normalization instead of instance normalization.
       <br><br>
    The three animation style datasets used by AnimeGANv2 are shown in Table 1. The image size used in the training is 256*256, and these style images are all from the video frames in the corresponding high-definition style movies. Figure 3 shows the qualitative results of AnimeGANv2 on the test dataset for the three animation styles.
</div>
<p></p>
<div class="img2"><img src="./assets/G.jpg"></div>
<div class="longcaption">Figure 2. Architecture of the generator network.</div>

<p></p>
<div class="longcaption"  style="text-align:center">Table 1. Information of three anime style datasets.</div>
<div class="table" ><table border="1px" style="margin:0 auto; border-collapse:collapse; padding:4px">
	<tr align="center">
	    <th>Anime style</th>
	    <th>Film</th>  
	    <th>Picture Number</th>  
      <th>Quality</th>
      <th>Download Style Dataset</th>
	</tr >
	<tr align="center">
      <td>Miyazaki Hayao</td>
      <td>The Wind Rises</td>
      <td>1752</td>
      <td>1080p</td>
	    <td rowspan="3"><a href="https://github.com/TachibanaYoshino/AnimeGANv2/releases/tag/1.0">Link</a></td>
	</tr>
	<tr align="center">
	    <td>Makoto Shinkai</td>  
	    <td>Your Name & Weathering with you</td>
      <td>1445</td>
      <td>BD</td>
	</tr>
	<tr align="center">
	    <td>Kon Satoshi</td>
	    <td>Paprika</td>
      <td>1284</td>
      <td>BDRip</td>
	</tr>
</table>  </div>

<p></p>
<div class="img3"><img src="./assets/AnimeGANv2-1.jpg"></div>
<div class="longcaption">Figure 3. The qualitative results of AnimeGANv2 on the three animation styles on the test dataset.</div>

<div class="section">See Also</div>
<ul>
<li> <a href="https://github.com/TachibanaYoshino/AnimeGANv2">Source Code</a> </li>
<li> <a href="#">Online Use (TBD)</a> </li>
<li> <a href="https://www.bilibili.com/video/BV13T4y1773X/">Demo Video</a> </li>
</ul>

<div class="section">Citation</div>
<p>
Jie Chen, Gang Liu, Xin Chen<br>
<a href="https://link.springer.com/chapter/10.1007/978-981-15-5577-0_18">"AnimeGAN: A Novel Lightweight GAN for Photo Animation."</a><br>
<i>ISICA 2019: Artificial Intelligence Algorithms and Applications pp 242-256</i>, 2019.

<div class="bottom">「AnimeGANv2」 Xin Chen, August, 2020</div>
</div>
</body></html>
